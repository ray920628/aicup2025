{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGBYZnV_f32q"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dyiXt-ChvAT"
   },
   "source": [
    "# Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyxunIpsFyY_"
   },
   "outputs": [],
   "source": [
    "!pip install datasets evaluate jiwer librosa\n",
    "!pip install --upgrade bitsandbytes transformers==4.50.0 accelerate\n",
    "!pip install ctranslate2==4.4.0 whisperx\n",
    "!apt-get install libcudnn8 libcudnn8-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c-1H4M71YJw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import whisperx\n",
    "from tqdm import tqdm  # 進度條\n",
    "\n",
    "def extract_number(f):\n",
    "    \"\"\"\n",
    "    從檔名中抓出第一個數字，用來排序。\n",
    "    若找不到數字，就回傳無限大 (讓它排在最後)。\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d+', os.path.basename(f))\n",
    "    return int(match.group()) if match else float('inf')\n",
    "\n",
    "# 1. 找出所有 .wav 檔案，並依照檔名中的數字排序\n",
    "audio_folder = \"/content/drive/MyDrive/Colab Notebooks/private\"\n",
    "audio_files = sorted(\n",
    "    glob.glob(os.path.join(audio_folder, \"*.wav\")),\n",
    "    key=extract_number\n",
    ")\n",
    "print(f\"找到 {len(audio_files)} 個 .wav 檔案。\")\n",
    "\n",
    "# 2. 決定要用 CPU 還是 GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用裝置：{device}\")\n",
    "\n",
    "# 3. 載入 WhisperX 語音模型\n",
    "whisperx_model = whisperx.load_model(\"large-v3\", device, compute_type = \"float16\")\n",
    "\n",
    "# 4. 用來儲存最終結果的 list（給 JSON）\n",
    "results = []\n",
    "\n",
    "# === 用來儲存 task1 的格式輸出 ===\n",
    "task1_lines = []\n",
    "\n",
    "# 5. 處理所有音檔\n",
    "for idx, audio_path in enumerate(tqdm(audio_files, desc=\"處理進度\")):\n",
    "    file_id = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    print(f\"\\n>>> 正在處理檔案 {idx+1}/{len(audio_files)}：{file_id}.wav\")\n",
    "\n",
    "    # 5.1 用 WhisperX 轉錄\n",
    "    raw_result = whisperx_model.transcribe(audio_path)\n",
    "    segments = raw_result[\"segments\"]\n",
    "\n",
    "    # 5.2 自動偵測語言並載入對齊模型\n",
    "    lang_code = raw_result[\"language\"]\n",
    "    print(f\"偵測到語言: {lang_code}\")\n",
    "\n",
    "    align_model, metadata = whisperx.load_align_model(lang_code, device)\n",
    "\n",
    "    # 5.3 進行詞級對齊\n",
    "    aligned_result = whisperx.align(\n",
    "        segments,\n",
    "        align_model,\n",
    "        metadata,\n",
    "        audio_path,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # 5.4 組成 words_info\n",
    "    words_info = []\n",
    "    for w in aligned_result[\"word_segments\"]:\n",
    "        word_text = w[\"word\"]\n",
    "        start_time = float(w[\"start\"])\n",
    "        end_time = float(w[\"end\"])\n",
    "        words_info.append({\n",
    "            \"word\": word_text,\n",
    "            \"start\": start_time,\n",
    "            \"end\": end_time\n",
    "        })\n",
    "\n",
    "    # 5.5 串成完整文字（加空格）\n",
    "    full_text = \" \".join([w[\"word\"] for w in words_info]).strip()\n",
    "\n",
    "    # 5.6 統計資訊\n",
    "    print(f\"　- 偵測語言：{lang_code}\")\n",
    "    print(f\"　- 轉錄詞數：{len(words_info)}\")\n",
    "    print(f\"　- 轉錄文字長度：{len(full_text)} 字元\")\n",
    "\n",
    "    # 5.7 結果加入 JSON 結構\n",
    "    results.append({\n",
    "        \"file_id\": file_id,\n",
    "        \"language\": lang_code,\n",
    "        \"words\": words_info,\n",
    "        \"text\": full_text\n",
    "    })\n",
    "\n",
    "    # 5.8 結果加入 task1_lines\n",
    "    task1_lines.append(f\"{file_id}\\t{full_text}\")\n",
    "\n",
    "# 6. 輸出 JSON\n",
    "output_json_path = \"/content/drive/MyDrive/Colab Notebooks/aicup0607/transcription_results_whisperx.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\n✅ 已儲存所有轉錄與對齊結果到：{output_json_path}\")\n",
    "\n",
    "# 7. 輸出 task1 格式 TXT\n",
    "output_txt_path = \"/content/drive/MyDrive/Colab Notebooks/aicup0607/task1_output_whisperx.txt\"\n",
    "with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(task1_lines))\n",
    "print(f\"✅ 已輸出比賽格式 TXT 檔至：{output_txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzV3_cZijPui"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"YOUR_HUGGINGFACE_TOKEN\")  # 替換成你的 Hugging Face Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qEbiUqMDRmfl"
   },
   "outputs": [],
   "source": [
    "# 1️⃣ 安裝\n",
    "!pip install --upgrade pip\n",
    "!pip install torch transformers accelerate\n",
    "\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 2️⃣ 模型載入\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQIhCNdSw5kt"
   },
   "outputs": [],
   "source": [
    "# 3️⃣ Prompt 建構\n",
    "def build_prompt(text):\n",
    "    labels = [\n",
    "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"FAMILYNAME\", \"PERSONALNAME\", \"PROFESSION\",\n",
    "        \"ROOM\", \"DEPARTMENT\", \"HOSPITAL\", \"ORGANIZATION\", \"STREET\", \"CITY\",\n",
    "        \"DISTRICT\", \"COUNTY\", \"STATE\", \"COUNTRY\", \"ZIP\", \"LOCATION-OTHER\", \"AGE\",\n",
    "        \"DATE\", \"TIME\", \"DURATION\", \"SET\", \"PHONE\", \"FAX\", \"EMAIL\", \"URL\",\n",
    "        \"IPADDRESS\", \"SOCIAL_SECURITY_NUMBER\", \"MEDICAL_RECORD_NUMBER\",\n",
    "        \"HEALTH_PLAN_NUMBER\", \"ACCOUNT_NUMBER\", \"LICENSE_NUMBER\", \"VEHICLE_ID\",\n",
    "        \"DEVICE_ID\", \"BIOMETRIC_ID\", \"ID_NUMBER\", \"OTHER\"\n",
    "    ]\n",
    "    label_str = \", \".join(labels)\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "你是一個醫療病例報告SHI檢測模型，請從以下輸入文本中抽取SHI類別的實體，並且只使用以下的label：\n",
    "{label_str}。\n",
    "\"\"\".strip()\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "請找出文本中的SHI，並只能輸出 JSON 格式，每個label對應的值放在JSON中，每個元素包含\"label\"與\"entity_text\"兩個欄位。若在文字中只要有可能符合，請務必標註，不要輸出空陣列。以下是範例\n",
    "輸入:\"A 69-year-old patient Jack Bryant, identified by Episode Number 27O537406U and Medical Record 2755374.ARU, resides on Sandering Street in Barwon Heads, Western Australia, with a ZIP code of 6906.\"\n",
    "輸出:\n",
    "[\n",
    "  {{\"label\": \"AGE\", \"entity_text\": \"69\"}},\n",
    "  {{\"label\": \"PATIENT\", \"entity_text\": \"Jack Bryant\"}},\n",
    "  {{\"label\": \"MEDICAL_RECORD_NUMBER\", \"entity_text\": \"2755374.ARU\"}},\n",
    "  {{\"label\": \"STREET\", \"entity_text\": \"Sandering\"}},\n",
    "  {{\"label\": \"CITY\", \"entity_text\": \"Barwon Heads\"}},\n",
    "  {{\"label\": \"STATE\", \"entity_text\": \"Western Australia\"}}\n",
    "]：\n",
    "{text}\n",
    "\"\"\".strip()\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "# 4️⃣ WhisperX JSON 載入\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/transcription_results_whisperx.json', 'r', encoding='utf-8') as f:\n",
    "    whisper_data = json.load(f)\n",
    "\n",
    "# 5️⃣ LLM NER 辨識\n",
    "def run_ner(text):\n",
    "    system_prompt, user_prompt = build_prompt(text)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    result_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # 先移除多餘的 [INST] / [/INST] 區塊\n",
    "    result_text = re.sub(r'\\[INST\\][\\s\\S]*?\\[/INST\\]', '', result_text).strip()\n",
    "    # 👀 顯示模型回答\n",
    "    print(\"📢 模型回答：\")\n",
    "    print(result_text)\n",
    "    print(\"=\" * 28)\n",
    "\n",
    "    # 嘗試提取 JSON 區塊\n",
    "    json_candidates = re.findall(r'\\[[\\s\\S]*?\\]', result_text)\n",
    "    if json_candidates:\n",
    "        json_str = json_candidates[-1]\n",
    "    else:\n",
    "        print(\"⚠️ 找不到 JSON 區塊，以下為原始輸出：\")\n",
    "        print(result_text)\n",
    "        json_str = \"[]\"\n",
    "\n",
    "    # JSON 解析\n",
    "    try:\n",
    "        parsed_output = json.loads(json_str)\n",
    "        print(f\"✅ JSON解析成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ JSON解析失敗：{e}\")\n",
    "        print(f\"模型輸出：{json_str}\")\n",
    "        parsed_output = []\n",
    "    return parsed_output\n",
    "\n",
    "# 6️⃣ NER結果產生\n",
    "ner_results = {}\n",
    "for entry in whisper_data:\n",
    "    file_id = entry.get('file_id') or entry.get('id') or 'unknown'\n",
    "    text = entry['text']\n",
    "    ner_results[file_id] = run_ner(text)\n",
    "\n",
    "# 7️⃣ 儲存ner_results.json\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/ner_results.json', 'w', encoding='utf-8') as fout:\n",
    "    json.dump(ner_results, fout, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ 已完成 NER 辨識，結果已存至 ner_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOYZT5Nucoy0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/transcription_results_whisperx.json', 'r', encoding='utf-8') as f:\n",
    "    whisper_data = json.load(f)\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/ner_results.json', 'r', encoding='utf-8') as f:\n",
    "    ner_results = json.load(f)\n",
    "\n",
    "# 8️⃣ 對齊時間戳\n",
    "def align_entity_to_time(entity_text, words):\n",
    "    cleaned_entity = re.sub(r'[^\\w\\s]', '', entity_text).lower()\n",
    "    entity_tokens = cleaned_entity.split()\n",
    "    for i in range(len(words) - len(entity_tokens) + 1):\n",
    "        segment = ' '.join(\n",
    "            re.sub(r'[^\\w\\s]', '', words[j]['word']).lower()\n",
    "            for j in range(i, i + len(entity_tokens))\n",
    "        )\n",
    "        if segment == ' '.join(entity_tokens):\n",
    "            start_time = words[i]['start']\n",
    "            end_time = words[i + len(entity_tokens) - 1]['end']\n",
    "            return start_time, end_time\n",
    "    return None, None\n",
    "\n",
    "# 9️⃣ 輸出task2_answer.txt\n",
    "output_lines = []\n",
    "for entry in whisper_data:\n",
    "    file_id = entry.get('file_id') or entry.get('id') or 'unknown'\n",
    "    words = entry['words']\n",
    "    entities = ner_results.get(file_id, [])\n",
    "\n",
    "    for entity in entities:\n",
    "        # Check if the entity is a dictionary before accessing keys\n",
    "        if isinstance(entity, dict):\n",
    "            label = entity.get('label')\n",
    "            entity_text = entity.get('entity_text')\n",
    "\n",
    "            # 檢查 label 和 entity_text 是否都存在且 entity_text 不為空\n",
    "            if label is None or entity_text is None or not entity_text.strip():\n",
    "                 print(f\"⚠️ file_id: {file_id} 中的實體資料不完整 ({entity})，已略過\")\n",
    "                 continue\n",
    "\n",
    "            start_time, end_time = align_entity_to_time(entity_text, words)\n",
    "            if start_time is not None and end_time is not None:\n",
    "                line = f\"{file_id}\\t{label}\\t{start_time:.3f}\\t{end_time:.3f}\\t{entity_text}\"\n",
    "                output_lines.append(line)\n",
    "            else:\n",
    "                print(f\"⚠️ 在 {file_id} 找不到對應時間戳，略過 -> {entity_text}\")\n",
    "        else:\n",
    "            print(f\"⚠️ file_id: {file_id} 中的實體不是預期的字典格式 ({entity}, type: {type(entity)})，已略過\")\n",
    "\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/task2_output.txt', 'w', encoding='utf-8') as fout:\n",
    "    fout.write('\\n'.join(output_lines))\n",
    "\n",
    "print(\"✅ 已完成 task2_answer.txt 的生成！\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMMtYgkDH7vk21hA922BII",
   "collapsed_sections": [
    "tUqKhoTcrDRQ",
    "jiIqNNfHfxnu"
   ],
   "gpuType": "T4",
   "mount_file_id": "1YRaGHy92bzITbGhEgbj_HYomwvdd09VT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
