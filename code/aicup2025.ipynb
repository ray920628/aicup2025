{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGBYZnV_f32q"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dyiXt-ChvAT"
   },
   "source": [
    "# Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyxunIpsFyY_"
   },
   "outputs": [],
   "source": [
    "!pip install datasets evaluate jiwer librosa\n",
    "!pip install --upgrade bitsandbytes transformers==4.50.0 accelerate\n",
    "!pip install ctranslate2==4.4.0 whisperx\n",
    "!apt-get install libcudnn8 libcudnn8-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c-1H4M71YJw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import whisperx\n",
    "from tqdm import tqdm  # é€²åº¦æ¢\n",
    "\n",
    "def extract_number(f):\n",
    "    \"\"\"\n",
    "    å¾æª”åä¸­æŠ“å‡ºç¬¬ä¸€å€‹æ•¸å­—ï¼Œç”¨ä¾†æ’åºã€‚\n",
    "    è‹¥æ‰¾ä¸åˆ°æ•¸å­—ï¼Œå°±å›å‚³ç„¡é™å¤§ (è®“å®ƒæ’åœ¨æœ€å¾Œ)ã€‚\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d+', os.path.basename(f))\n",
    "    return int(match.group()) if match else float('inf')\n",
    "\n",
    "# 1. æ‰¾å‡ºæ‰€æœ‰ .wav æª”æ¡ˆï¼Œä¸¦ä¾ç…§æª”åä¸­çš„æ•¸å­—æ’åº\n",
    "audio_folder = \"/content/drive/MyDrive/Colab Notebooks/private\"\n",
    "audio_files = sorted(\n",
    "    glob.glob(os.path.join(audio_folder, \"*.wav\")),\n",
    "    key=extract_number\n",
    ")\n",
    "print(f\"æ‰¾åˆ° {len(audio_files)} å€‹ .wav æª”æ¡ˆã€‚\")\n",
    "\n",
    "# 2. æ±ºå®šè¦ç”¨ CPU é‚„æ˜¯ GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ä½¿ç”¨è£ç½®ï¼š{device}\")\n",
    "\n",
    "# 3. è¼‰å…¥ WhisperX èªéŸ³æ¨¡å‹\n",
    "whisperx_model = whisperx.load_model(\"large-v3\", device, compute_type = \"float16\")\n",
    "\n",
    "# 4. ç”¨ä¾†å„²å­˜æœ€çµ‚çµæœçš„ listï¼ˆçµ¦ JSONï¼‰\n",
    "results = []\n",
    "\n",
    "# === ç”¨ä¾†å„²å­˜ task1 çš„æ ¼å¼è¼¸å‡º ===\n",
    "task1_lines = []\n",
    "\n",
    "# 5. è™•ç†æ‰€æœ‰éŸ³æª”\n",
    "for idx, audio_path in enumerate(tqdm(audio_files, desc=\"è™•ç†é€²åº¦\")):\n",
    "    file_id = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    print(f\"\\n>>> æ­£åœ¨è™•ç†æª”æ¡ˆ {idx+1}/{len(audio_files)}ï¼š{file_id}.wav\")\n",
    "\n",
    "    # 5.1 ç”¨ WhisperX è½‰éŒ„\n",
    "    raw_result = whisperx_model.transcribe(audio_path)\n",
    "    segments = raw_result[\"segments\"]\n",
    "\n",
    "    # 5.2 è‡ªå‹•åµæ¸¬èªè¨€ä¸¦è¼‰å…¥å°é½Šæ¨¡å‹\n",
    "    lang_code = raw_result[\"language\"]\n",
    "    print(f\"åµæ¸¬åˆ°èªè¨€: {lang_code}\")\n",
    "\n",
    "    align_model, metadata = whisperx.load_align_model(lang_code, device)\n",
    "\n",
    "    # 5.3 é€²è¡Œè©ç´šå°é½Š\n",
    "    aligned_result = whisperx.align(\n",
    "        segments,\n",
    "        align_model,\n",
    "        metadata,\n",
    "        audio_path,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # 5.4 çµ„æˆ words_info\n",
    "    words_info = []\n",
    "    for w in aligned_result[\"word_segments\"]:\n",
    "        word_text = w[\"word\"]\n",
    "        start_time = float(w[\"start\"])\n",
    "        end_time = float(w[\"end\"])\n",
    "        words_info.append({\n",
    "            \"word\": word_text,\n",
    "            \"start\": start_time,\n",
    "            \"end\": end_time\n",
    "        })\n",
    "\n",
    "    # 5.5 ä¸²æˆå®Œæ•´æ–‡å­—ï¼ˆåŠ ç©ºæ ¼ï¼‰\n",
    "    full_text = \" \".join([w[\"word\"] for w in words_info]).strip()\n",
    "\n",
    "    # 5.6 çµ±è¨ˆè³‡è¨Š\n",
    "    print(f\"ã€€- åµæ¸¬èªè¨€ï¼š{lang_code}\")\n",
    "    print(f\"ã€€- è½‰éŒ„è©æ•¸ï¼š{len(words_info)}\")\n",
    "    print(f\"ã€€- è½‰éŒ„æ–‡å­—é•·åº¦ï¼š{len(full_text)} å­—å…ƒ\")\n",
    "\n",
    "    # 5.7 çµæœåŠ å…¥ JSON çµæ§‹\n",
    "    results.append({\n",
    "        \"file_id\": file_id,\n",
    "        \"language\": lang_code,\n",
    "        \"words\": words_info,\n",
    "        \"text\": full_text\n",
    "    })\n",
    "\n",
    "    # 5.8 çµæœåŠ å…¥ task1_lines\n",
    "    task1_lines.append(f\"{file_id}\\t{full_text}\")\n",
    "\n",
    "# 6. è¼¸å‡º JSON\n",
    "output_json_path = \"/content/drive/MyDrive/Colab Notebooks/aicup0607/transcription_results_whisperx.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nâœ… å·²å„²å­˜æ‰€æœ‰è½‰éŒ„èˆ‡å°é½Šçµæœåˆ°ï¼š{output_json_path}\")\n",
    "\n",
    "# 7. è¼¸å‡º task1 æ ¼å¼ TXT\n",
    "output_txt_path = \"/content/drive/MyDrive/Colab Notebooks/aicup0607/task1_output_whisperx.txt\"\n",
    "with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(task1_lines))\n",
    "print(f\"âœ… å·²è¼¸å‡ºæ¯”è³½æ ¼å¼ TXT æª”è‡³ï¼š{output_txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzV3_cZijPui"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"YOUR_HUGGINGFACE_TOKEN\")  # æ›¿æ›æˆä½ çš„ Hugging Face Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qEbiUqMDRmfl"
   },
   "outputs": [],
   "source": [
    "# 1ï¸âƒ£ å®‰è£\n",
    "!pip install --upgrade pip\n",
    "!pip install torch transformers accelerate\n",
    "\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 2ï¸âƒ£ æ¨¡å‹è¼‰å…¥\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQIhCNdSw5kt"
   },
   "outputs": [],
   "source": [
    "# 3ï¸âƒ£ Prompt å»ºæ§‹\n",
    "def build_prompt(text):\n",
    "    labels = [\n",
    "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"FAMILYNAME\", \"PERSONALNAME\", \"PROFESSION\",\n",
    "        \"ROOM\", \"DEPARTMENT\", \"HOSPITAL\", \"ORGANIZATION\", \"STREET\", \"CITY\",\n",
    "        \"DISTRICT\", \"COUNTY\", \"STATE\", \"COUNTRY\", \"ZIP\", \"LOCATION-OTHER\", \"AGE\",\n",
    "        \"DATE\", \"TIME\", \"DURATION\", \"SET\", \"PHONE\", \"FAX\", \"EMAIL\", \"URL\",\n",
    "        \"IPADDRESS\", \"SOCIAL_SECURITY_NUMBER\", \"MEDICAL_RECORD_NUMBER\",\n",
    "        \"HEALTH_PLAN_NUMBER\", \"ACCOUNT_NUMBER\", \"LICENSE_NUMBER\", \"VEHICLE_ID\",\n",
    "        \"DEVICE_ID\", \"BIOMETRIC_ID\", \"ID_NUMBER\", \"OTHER\"\n",
    "    ]\n",
    "    label_str = \", \".join(labels)\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹é†«ç™‚ç—…ä¾‹å ±å‘ŠSHIæª¢æ¸¬æ¨¡å‹ï¼Œè«‹å¾ä»¥ä¸‹è¼¸å…¥æ–‡æœ¬ä¸­æŠ½å–SHIé¡åˆ¥çš„å¯¦é«”ï¼Œä¸¦ä¸”åªä½¿ç”¨ä»¥ä¸‹çš„labelï¼š\n",
    "{label_str}ã€‚\n",
    "\"\"\".strip()\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "è«‹æ‰¾å‡ºæ–‡æœ¬ä¸­çš„SHIï¼Œä¸¦åªèƒ½è¼¸å‡º JSON æ ¼å¼ï¼Œæ¯å€‹labelå°æ‡‰çš„å€¼æ”¾åœ¨JSONä¸­ï¼Œæ¯å€‹å…ƒç´ åŒ…å«\"label\"èˆ‡\"entity_text\"å…©å€‹æ¬„ä½ã€‚è‹¥åœ¨æ–‡å­—ä¸­åªè¦æœ‰å¯èƒ½ç¬¦åˆï¼Œè«‹å‹™å¿…æ¨™è¨»ï¼Œä¸è¦è¼¸å‡ºç©ºé™£åˆ—ã€‚ä»¥ä¸‹æ˜¯ç¯„ä¾‹\n",
    "è¼¸å…¥:\"A 69-year-old patient Jack Bryant, identified by Episode Number 27O537406U and Medical Record 2755374.ARU, resides on Sandering Street in Barwon Heads, Western Australia, with a ZIP code of 6906.\"\n",
    "è¼¸å‡º:\n",
    "[\n",
    "  {{\"label\": \"AGE\", \"entity_text\": \"69\"}},\n",
    "  {{\"label\": \"PATIENT\", \"entity_text\": \"Jack Bryant\"}},\n",
    "  {{\"label\": \"MEDICAL_RECORD_NUMBER\", \"entity_text\": \"2755374.ARU\"}},\n",
    "  {{\"label\": \"STREET\", \"entity_text\": \"Sandering\"}},\n",
    "  {{\"label\": \"CITY\", \"entity_text\": \"Barwon Heads\"}},\n",
    "  {{\"label\": \"STATE\", \"entity_text\": \"Western Australia\"}}\n",
    "]ï¼š\n",
    "{text}\n",
    "\"\"\".strip()\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "# 4ï¸âƒ£ WhisperX JSON è¼‰å…¥\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/transcription_results_whisperx.json', 'r', encoding='utf-8') as f:\n",
    "    whisper_data = json.load(f)\n",
    "\n",
    "# 5ï¸âƒ£ LLM NER è¾¨è­˜\n",
    "def run_ner(text):\n",
    "    system_prompt, user_prompt = build_prompt(text)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    result_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # å…ˆç§»é™¤å¤šé¤˜çš„ [INST] / [/INST] å€å¡Š\n",
    "    result_text = re.sub(r'\\[INST\\][\\s\\S]*?\\[/INST\\]', '', result_text).strip()\n",
    "    # ğŸ‘€ é¡¯ç¤ºæ¨¡å‹å›ç­”\n",
    "    print(\"ğŸ“¢ æ¨¡å‹å›ç­”ï¼š\")\n",
    "    print(result_text)\n",
    "    print(\"=\" * 28)\n",
    "\n",
    "    # å˜—è©¦æå– JSON å€å¡Š\n",
    "    json_candidates = re.findall(r'\\[[\\s\\S]*?\\]', result_text)\n",
    "    if json_candidates:\n",
    "        json_str = json_candidates[-1]\n",
    "    else:\n",
    "        print(\"âš ï¸ æ‰¾ä¸åˆ° JSON å€å¡Šï¼Œä»¥ä¸‹ç‚ºåŸå§‹è¼¸å‡ºï¼š\")\n",
    "        print(result_text)\n",
    "        json_str = \"[]\"\n",
    "\n",
    "    # JSON è§£æ\n",
    "    try:\n",
    "        parsed_output = json.loads(json_str)\n",
    "        print(f\"âœ… JSONè§£ææˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ JSONè§£æå¤±æ•—ï¼š{e}\")\n",
    "        print(f\"æ¨¡å‹è¼¸å‡ºï¼š{json_str}\")\n",
    "        parsed_output = []\n",
    "    return parsed_output\n",
    "\n",
    "# 6ï¸âƒ£ NERçµæœç”¢ç”Ÿ\n",
    "ner_results = {}\n",
    "for entry in whisper_data:\n",
    "    file_id = entry.get('file_id') or entry.get('id') or 'unknown'\n",
    "    text = entry['text']\n",
    "    ner_results[file_id] = run_ner(text)\n",
    "\n",
    "# 7ï¸âƒ£ å„²å­˜ner_results.json\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/ner_results.json', 'w', encoding='utf-8') as fout:\n",
    "    json.dump(ner_results, fout, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… å·²å®Œæˆ NER è¾¨è­˜ï¼Œçµæœå·²å­˜è‡³ ner_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOYZT5Nucoy0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/transcription_results_whisperx.json', 'r', encoding='utf-8') as f:\n",
    "    whisper_data = json.load(f)\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/ner_results.json', 'r', encoding='utf-8') as f:\n",
    "    ner_results = json.load(f)\n",
    "\n",
    "# 8ï¸âƒ£ å°é½Šæ™‚é–“æˆ³\n",
    "def align_entity_to_time(entity_text, words):\n",
    "    cleaned_entity = re.sub(r'[^\\w\\s]', '', entity_text).lower()\n",
    "    entity_tokens = cleaned_entity.split()\n",
    "    for i in range(len(words) - len(entity_tokens) + 1):\n",
    "        segment = ' '.join(\n",
    "            re.sub(r'[^\\w\\s]', '', words[j]['word']).lower()\n",
    "            for j in range(i, i + len(entity_tokens))\n",
    "        )\n",
    "        if segment == ' '.join(entity_tokens):\n",
    "            start_time = words[i]['start']\n",
    "            end_time = words[i + len(entity_tokens) - 1]['end']\n",
    "            return start_time, end_time\n",
    "    return None, None\n",
    "\n",
    "# 9ï¸âƒ£ è¼¸å‡ºtask2_answer.txt\n",
    "output_lines = []\n",
    "for entry in whisper_data:\n",
    "    file_id = entry.get('file_id') or entry.get('id') or 'unknown'\n",
    "    words = entry['words']\n",
    "    entities = ner_results.get(file_id, [])\n",
    "\n",
    "    for entity in entities:\n",
    "        # Check if the entity is a dictionary before accessing keys\n",
    "        if isinstance(entity, dict):\n",
    "            label = entity.get('label')\n",
    "            entity_text = entity.get('entity_text')\n",
    "\n",
    "            # æª¢æŸ¥ label å’Œ entity_text æ˜¯å¦éƒ½å­˜åœ¨ä¸” entity_text ä¸ç‚ºç©º\n",
    "            if label is None or entity_text is None or not entity_text.strip():\n",
    "                 print(f\"âš ï¸ file_id: {file_id} ä¸­çš„å¯¦é«”è³‡æ–™ä¸å®Œæ•´ ({entity})ï¼Œå·²ç•¥é\")\n",
    "                 continue\n",
    "\n",
    "            start_time, end_time = align_entity_to_time(entity_text, words)\n",
    "            if start_time is not None and end_time is not None:\n",
    "                line = f\"{file_id}\\t{label}\\t{start_time:.3f}\\t{end_time:.3f}\\t{entity_text}\"\n",
    "                output_lines.append(line)\n",
    "            else:\n",
    "                print(f\"âš ï¸ åœ¨ {file_id} æ‰¾ä¸åˆ°å°æ‡‰æ™‚é–“æˆ³ï¼Œç•¥é -> {entity_text}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ file_id: {file_id} ä¸­çš„å¯¦é«”ä¸æ˜¯é æœŸçš„å­—å…¸æ ¼å¼ ({entity}, type: {type(entity)})ï¼Œå·²ç•¥é\")\n",
    "\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/aicup0607/task2_output.txt', 'w', encoding='utf-8') as fout:\n",
    "    fout.write('\\n'.join(output_lines))\n",
    "\n",
    "print(\"âœ… å·²å®Œæˆ task2_answer.txt çš„ç”Ÿæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMMtYgkDH7vk21hA922BII",
   "collapsed_sections": [
    "tUqKhoTcrDRQ",
    "jiIqNNfHfxnu"
   ],
   "gpuType": "T4",
   "mount_file_id": "1YRaGHy92bzITbGhEgbj_HYomwvdd09VT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
